---
title: "A Bayesian Analysis of the Determinants of House Prices"
date: 2019-05-16
tags: [Statistical Inference, Bayesian Statistics]
excerpt: "Using Bayesian Linear Regression to Estimate the Determinants of House Prices in the BostonHousing Dataset"
---

## Objective:

Housing prices are a trendy news topic right now, and Bayesian statistics is a trendy field in stats, so let's put them together and be double trendy. In this post, I will be using Bayesian linear regression to estimate the microeconomic factors that determine house prices.

To do this, we'll be using the popular BostonHousing dataset from Harrison and Rubinfeld (1979), which features information for 506 census tracts of Boston from the 1970 census.

## Analysis:

To run the Baysian linear regression, I will be using Markov Chain Monte Carlo (MCMC) methods in the WinBUGS statistical software. WinBUGS allows us to estimate our posterior distributions for the model parameters using Gibbs sampling, and can conveniently be run through R using the R2OpenBUGS package.

```r
library(mlbench)
library(R2OpenBUGS)
library(coda)
```

Let's start off by loading our dataset, defining our variables, and putting it into a format readable by WinBUGS.

```r
WorkDir<- "C:\\Users\\Viktor\\Desktop"
setwd(WorkDir)
set.seed(42)

# load our dataset
data(BostonHousing)
dat = BostonHousing

J <- nrow(dat)
y <- dat$medv # median value of owner-occupied homes in USD 1000's
crim <- dat$crim # per capita crime rate by town
zn <- dat$zn # 	proportion of residential land zoned for lots over 25,000 sq.ft
indus <- dat$indus # proportion of non-retail business acres per town
chas <- as.numeric(dat$chas) # Charles River dummy variable
nox <- dat$nox # nitric oxides concentration (parts per 10 million)
rm <- dat$rm # average number of rooms per dwelling
age <- dat$age # proportion of owner-occupied units built prior to 1940
dis <- dat$dis # weighted distances to five Boston employment centres
rad <- dat$rad # index of accessibility to radial highways
tax <- dat$tax # full-value property-tax rate per USD 10,000
ptratio <- dat$ptratio # pupil-teacher ratio by town
b <- dat$b # 1000(B - 0.63)^2 where B is the proportion of town population who identify as black
lstat <- dat$lstat # 	percentage of lower status of the population

bug.dat <- list("J", "y", "crim", "zn", "indus", "chas", "nox", "rm", "age", "dis", "rad",    
             "tax", "ptratio", "b", "lstat")
```

Now we will write the model out in BUGS format as a .txt file so we can call it later. We'll keep the model simple and use very low-information priors. Our dependent variable y[j] will follow a normal distribution with mean mu[j] and precision tau. For tau, we'll assign a gamma(.5,.01) distribution. We'll allow our beta coefficients to follow a normal distribution with 0 mean and precision of 0.0625, again keeping with our theme of low-information priors.

```r
cat("
    model {
    for (j in 1:J)
    {
    y[j] ~ dnorm(mu[j], tau)
    mu[j] <- B[1]*crim[j] + B[2]*zn[j] + B[3]*indus[j] + B[4]*chas[j] +
    B[5]*nox[j] + B[6]*rm[j] + B[7]*age[j] + B[8]*dis[j] + B[9]*rad[j] + 
    B[10]*tax[j] + B[11]*ptratio[j] + B[12]*b[j] + B[13]*lstat[j] + B[14]
    }

    tau~dgamma(.5,.01)
    for(i in 1:14){
    B[i]~dnorm(0,0.0625)}
    }", file="BostonModel.txt")
```

We then set initial values for our MCMC algorithm, specify the parameters of interest that we'd like to keep track of, and then run our model.

We'll run three chains for 20,000 total iterations, burning the first 5000 and thinning slightly. Throwing out the first 5000 observations from our MCMC ensures we only look at observations after the algorithm has converged, and don't get misleading results based on our arbitrary choice of MCMC starting values.

Thinning provides the advantages of simplicity and a reduction in memory usage, therefore making models faster to run. The disadvantage is that you lose efficiency due to throwing away useful information compared to using all iterations. Running for 20,000 iterations should give us plenty to work with though, so we'll stick with quality over quantity here.

```r
inits<-function(){ list(B=rnorm(14), tau=runif(.5,1))} 
params=c("B", "tau")


housing.sim <- bugs(bug.dat, inits, model.file = "C:\\Users\\Viktor\\Desktop\\BostonModel.txt",
                  params, n.chains=3, n.iter=20000, n.burnin=5000, n.thin=3)

print(housing.sim, digits.summary = 3)
```

```r
#Output
Current: 3 chains, each with 20000 iterations (first 5000 discarded), n.thin = 3
Cumulative: n.sims = 45000 iterations saved
             mean    sd     2.5%      25%      50%      75%    97.5%   Rhat  n.eff
B[1]       -0.097  0.033   -0.163   -0.119   -0.097   -0.074   -0.031  1.001 19000
B[2]        0.048  0.014    0.021    0.039    0.048    0.058    0.076  1.001 12000
B[3]       -0.016  0.062   -0.136   -0.057   -0.016    0.025    0.106  1.003   980
B[4]        3.138  0.847    1.473    2.566    3.137    3.707    4.797  1.001  4600
B[5]       -5.313  2.685  -10.570   -7.150   -5.313   -3.483   -0.057  1.003   860
B[6]        4.910  0.351    4.232    4.679    4.901    5.133    5.652  1.004   580
B[7]       -0.008  0.013   -0.034   -0.017   -0.008    0.001    0.018  1.001  6300
B[8]       -1.140  0.193   -1.516   -1.270   -1.140   -1.009   -0.761  1.001  6400
B[9]        0.222  0.065    0.094    0.178    0.222    0.266    0.351  1.003   940
B[10]      -0.011  0.004   -0.018   -0.014   -0.011   -0.008   -0.004  1.003  1000
B[11]      -0.604  0.115   -0.836   -0.680   -0.603   -0.527   -0.384  1.002  2600
B[12]       0.012  0.003    0.007    0.011    0.012    0.014    0.018  1.002  3500
B[13]      -0.474  0.050   -0.571   -0.508   -0.474   -0.441   -0.377  1.003   800
B[14]      11.174  3.118    4.924    9.128   11.140   13.230   17.480  1.009  1200
tau         0.043  0.003    0.038    0.041    0.043    0.045    0.048  1.001  7800
deviance 3031.975  7.383 3019.000 3027.000 3031.000 3037.000 3048.000  1.002  2000

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = Dbar-Dhat)
pD = 13.870 and DIC = 3046.000
DIC is an estimate of expected predictive error (lower deviance is better).

```

![alt]({{ site.url }}{{ site.baseurl }}/assets/post-photos-housing/acf.jpeg)

![alt]({{ site.url }}{{ site.baseurl }}/assets/post-photos-housing/trace.jpeg)

![alt]({{ site.url }}{{ site.baseurl }}/assets/post-photos-housing/gelman.jpeg)

![alt]({{ site.url }}{{ site.baseurl }}/assets/post-photos-housing/post1.jpeg)

![alt]({{ site.url }}{{ site.baseurl }}/assets/post-photos-housing/post2.jpeg)

![alt]({{ site.url }}{{ site.baseurl }}/assets/post-photos-housing/post3.jpeg)

![alt]({{ site.url }}{{ site.baseurl }}/assets/post-photos-housing/post4.jpeg)


